# LLM Interview Evaluation Service

This service evaluates technical interviews using multiple LLM providers (OpenAI, Google Gemini, and DeepSeek via OpenRouter) with Supabase integration for data persistence.

## Setup

1. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Environment Variables**
   This service uses the global environment file from the parent project. All required API keys and configuration are already set in the global `.env` file, including:
   - `OPENAI_API_KEY`
   - `GOOGLE_API_KEY` 
   - `OPENROUTER_API_KEY`
   - `DEEPSEEK_API_KEY`
   - `SUPABASE_URL`
   - `SUPABASE_ANON_KEY`
   - `STORAGE_PATH`

3. **Database Setup**
   Run the SQL script in `sql/create_interviews_table.sql` in your Supabase SQL editor to create the required table.

## Configuration

The service uses `app/infrastructure/config.py` for all configuration. Key settings:

### LLM Models
- **OpenAI**: `gpt-5` (configurable via `OPENAI_MODEL`)
- **Google**: `gemini-2.5-pro` (configurable via `GEMINI_MODEL`)
- **DeepSeek**: `deepseek/deepseek-chat-v3.1` (configurable via `DEEPSEEK_MODEL`)

### Evaluation Parameters
- **Max Tokens**: 2000 (for detailed evaluations)
- **Temperature**: 0.3 (for consistent, focused responses)
- **Multiple Evaluations**: Enabled by default

### Data Storage
- **File Storage**: `./app/storage/interviews/` (from global STORAGE_PATH)
- **Database**: Supabase (configured via global environment)

## Usage

### Running the Service
```bash
python main.py
```

### Using File-Based Interview Evaluation
```python
from app.infrastructure.llm_provider import load_interview_from_source, run_evaluations

# Load interview data from JSON file
interview = load_interview_from_source('file', 'path/to/interview.json')

# Run evaluations with all three LLM providers
evaluated_interview = run_evaluations(interview)

# Access results
print(evaluated_interview.evaluation_1)  # OpenAI evaluation
print(evaluated_interview.evaluation_2)  # Gemini evaluation  
print(evaluated_interview.evaluation_3)  # DeepSeek evaluation
```

### Using Supabase Integration
```python
from app.infrastructure.llm_provider import load_interview_from_source, run_evaluations
from app.infrastructure.persistence.supabase.interview_repository import save_interview_to_supabase
from app.domain.entities.interview import Interview
import asyncio

# Create a new interview
interview = Interview(
    interview_id="unique-interview-id",
    system_prompt="Your evaluation prompt",
    rubric="Your rubric",
    jd="Job description", 
    full_transcript="Interview transcript"
)

# Save to Supabase
loop = asyncio.get_event_loop()
saved_interview = loop.run_until_complete(save_interview_to_supabase(interview))

# Load from Supabase
loaded_interview = load_interview_from_source('supabase', 'unique-interview-id')

# Run evaluations
evaluated_interview = run_evaluations(loaded_interview)

# Save the evaluated interview back to Supabase
final_interview = loop.run_until_complete(save_interview_to_supabase(evaluated_interview))
```

## Architecture

The service follows Clean Architecture principles:

```
app/
├── domain/           # Business logic and entities (Interview class)
├── application/      # Use cases (generated by LLM - not currently used)
└── infrastructure/   # External integrations
    ├── config.py     # Configuration management
    ├── llm_provider.py # LLM provider integrations
    ├── api/          # FastAPI routes and models
    └── persistence/  # Data persistence
        └── supabase/ # Supabase repository implementations
```

## Data Sources

The service supports multiple data sources:

1. **File-based**: Load interviews from JSON files
2. **Supabase**: Full CRUD operations with Supabase database
3. **Legacy 'db'**: Redirects to Supabase for backward compatibility

## Environment Variables

All configuration is inherited from the global environment file. Service-specific overrides can be added to the global `.env` file if needed.

## Ejecución con Docker

1. Construir la imagen:
```bash
docker build -t llm-service .
```

2. Ejecutar el contenedor:
```bash
docker run -p 8001:8001 llm-service
```

3. Verificar el estado:
```bash
curl http://localhost:8001/api/v1/healthz
```

## API Endpoints

- `GET /api/v1/healthz` - Health check
- `POST /api/v1/prompts` - Crear prompt
- `GET /api/v1/prompts` - Listar prompts
- `GET /api/v1/prompts/{id}` - Obtener prompt